# MLX Engine

A high-performance FastAPI server for running Large Language Models (LLMs), Vision Language Models (VLMs), and Text-to-Speech (TTS) models on Apple Silicon using [MLX](https://github.com/ml-explore/mlx).

## Features

- **ğŸš€ OpenAI-Compatible API (WIP)**: Implements OpenAI's chat completions format for easy integration 
- **ğŸ Apple Silicon Optimized**: Leverages MLX for efficient inference on Mac
- **ğŸ’¬ Chat Completions**: Full support for multi-turn conversations with system/user/assistant roles
- **ğŸ–¼ï¸ Vision Language Models**: Process images alongside text using VLMs
- **ğŸ“¡ Streaming Support**: Real-time token streaming for responsive applications
- **âš¡ Speculative Decoding**: Accelerate generation using a smaller draft model
- **ğŸ’¾ KV Cache Quantization**: Reduce memory usage with 3-8 bit quantization
- **ğŸ”Š Text-to-Speech**: Generate audio from text using MLX-based TTS models
- **ğŸ“¦ Model Management**: Download and convert Hugging Face models to MLX format

## Acknowledgements

This project is based on the open-source work from [LM Studio](https://lmstudio.ai/). Thank you to the LM Studio team!
